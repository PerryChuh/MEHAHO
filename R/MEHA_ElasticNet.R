#' Solving Elastic Net Problem Based on MEHA
#' @description Elastic Net combines the penalties of Lasso (L1 regularization)
#'     and Ridge (L2 regularization) methods. Thus the model introduces two
#'     hyperparameters: \code{x1}, which controls the strength of L1 regularization;
#'     and \code{x2}, which controls the strength of L2 regularization.
#'     The purpose of this function is to determine the optimal feature
#'     coefficients \code{y} and the hyperparameters \code{x1} and \code{x2} of the
#'     elastic net based on the input training and validation sets using MEHA.
#' @references Liu, R., Liu, Z., Yao, W., Zeng, S., & Zhang, J. (2024).
#'     "Rethinking Moreau Envelope for Nonconvex Bi-Level Optimization: A Single-loop and Hessian-free Solution Strategy."
#'     Available at: https://openreview.net/forum?id=i6EtCiIK4a
#'
#'     Gao, L., Ye, J. J., Yin, H., Zeng, S., & Zhang, J. (2022).
#'     "Value function based difference-of-convex algorithm for bilevel hyperparameter selection problems."
#'     Available at: https://proceedings.mlr.press/v162/gao22j.html
#' @param A_val Input feature matrix of the validation set, with dimensions n by p,
#'     where n is the total number of validation samples and p is the dimension of
#'     features. Each row represents an observation vector.
#' @param b_val Quantitative response variable of validation set.
#' @param A_tr Input feature matrix of training set, with dimension n' by p,
#'     where n' is the total number of training samples and p is the dimension of features.
#' @param b_tr Quantitative response variable of training set.
#' @param N Total iterations of MEHA. Default is 500.
#' @param alpha Projection stepsize of \code{x}, which is fixed. Default is 0.001.
#'     Specifically, you can edit the code and utilize the inverse power learning rate annealing strategy to dynamically adjust it.
#' @param beta  Proximal gradient stepsize of \code{y} which is fixed. Default is 1e-5.
#'     Specifically, you can edit the code and utilize the inverse power learning rate annealing strategy to dynamically adjust it.
#' @param eta Proximal gradient stepsize of \code{theta}, which is fixed.
#'     Default is 1e-5.
#' @param gamma Moreau envelope parameter. Default is 2.
#' @param c \eqn{\underline{c}} in MEHA, which is used to generate the penalty parameter \eqn{c_k} in LV-HBA by \eqn{c_k = \underline{c}(k+1)^p}.
#'     Default is 1.
#' @param p Default is 0.48.
#' @param tol Tolerance. IF \eqn{|x^{k + 1} - x^{k}|/|x^{k}| < tol}, then terminate the iteration, where \eqn{x^k} represents the upper-level variable.
#' @param auto_tuning When alpha, beta, eta are fixed, whether an auto-hyperparameter-tuning is needed.
#'     Default is \code{FALSE}.
#' @param temperature Temperature of simulating annealing method for auto-
#'     hyperparameter-tuning. Default is 0.1.
#'
#' @return
#'
#'   \item{x}{The first value is \code{x1} (Lasso penalty strength), while the
#'    second value is \code{x2} (Ridge penalty strength).}
#'   \item{y}{The feature coefficient vector, of dimension p, where p is the
#'       p is the dimension of features.}
#'   \item{theta}{The moreau envelope parameter in MEHA, which is of the same scale as variable y}
#'   \item{Xconv}{Describe the relative convergence situation of sequence x generated by MEHA,
#'       which is computed by \eqn{||x^{k+1}-x^k|| / ||x^k||} based on l2-norm.}
#'   \item{Yconv}{Describe the relative convergence situation of sequence y generated by MEHA,
#'       which is computed by \eqn{||y^{k+1}-y^k|| / ||y^k||} based on l2-norm.}
#'   \item{Thetaconv}{Describe the relative convergence situation of sequence theta generated by MEHA,
#'       which is computed by \eqn{||theta^{k+1}-theta^k|| / ||theta^k||} based on l2-norm.}
#'   \item{Fseq}{The upper function value sequence generated by the iteration based on validation set.}
#'
#'
#' @export
#'
#' @examples
#' library(MASS)
#' p <- 600  # Number of features
#' M <- 30  # Number of groups
#' n_tr <- 100  # Number of training samples
#' n_val <- 100  # Number of validation samples
#' n_test <- 100  # Number of test samples
#' signal_to_noise_ratio <- 2  # Signal-to-noise ratio
#'
#' # Generate correlation matrix
#' co_matrix <- diag(1, nrow = p, ncol = p)
#'
#' # Generate A matrix
#' A <- MASS::mvrnorm(n = n_tr + n_val + n_test, mu = rep(0, p), Sigma = co_matrix)
#'
#' # Generate beta vector
#' beta_1 <- matrix(rep(0, p / 3), ncol = 1)
#' for (i in 1:5) {
#'   beta_1[i, 1] <- i
#' }
#' beta <- rbind(beta_1, beta_1, beta_1)
#'
#' # Generate epsilon
#' epsilon <- matrix(rnorm(n_tr + n_val + n_test), ncol = 1)
#'
#' # Adjust sigma to achieve the specified signal-to-noise ratio
#' sigma <- norm(A %*% beta, type = "2") / (signal_to_noise_ratio * norm(epsilon, type = "2"))
#' b <- A %*% beta + sigma * epsilon
#'
#' # Split data into training, validation, and test sets
#' col_indices <- sample(n_tr + n_val + n_test)
#' A_tr <- A[col_indices[1:n_tr], ]
#' b_tr <- b[col_indices[1:n_tr], ]
#' A_val <- A[col_indices[(n_tr + 1):(n_tr + n_val)], ]
#' b_val <- b[col_indices[(n_tr + 1):(n_tr + n_val)], ]
#' A_test <- A[col_indices[(n_tr + n_val + 1):(n_tr + n_val + n_test)], ]
#' b_test <- b[col_indices[(n_tr + n_val + 1):(n_tr + n_val + n_test)], ]
#'
#'


MEHA_Elasticnet = function(A_val, b_val, A_tr, b_tr, N = 500, alpha = 1e-3, beta = 1e-5, eta = 1e-5, gamma = 2, c = 10, p = 0.48, tol = 0.05, auto_tuning = FALSE, temperature = 0.1){

  library(progress)
  library(truncnorm)


  main_fun <- function(A_val, b_val, A_tr, b_tr, N, alpha, beta, eta, gamma = gamma, c = c, p = p, tol = tol){

    p_fea = dim(A_val)[2]

    x = matrix(rep(0), nrow = 2)
    x[1, ] = 1
    x[2, ] = 0.5 # Initialize x
    y = matrix(rep(0), nrow = p_fea) # Initialize feature coefficients
    theta = y
    # theta = matrix(rep(0), nrow = p_fea)
    e2 = matrix(rep(0), nrow = 2)
    ep = matrix(rep(0), nrow = p_fea)

    # Upper objective function
    F = function(x,y){
      result = 0.5*norm(A_val %*% y - b_val, type = "2")^2
      return(result)
    }

    lower_fun = function(x, y){
      result = 0.5*norm(A_tr %*%  y - b_tr, type = "2")^2 + cbind(norm(y, type = "1"), 0.5*norm(y,type = "2")^2 ) %*% x
      return(result)
    }


    # Gradient update function for x and y
    F_x = function(x, y){
      result = t(matrix(rep(0, 2), ncol = 2))
      return(result)
    }

    F_y = function(x, y){
      result = t(t(A_val %*% y - b_val) %*% A_val)
      return(result)
    }

    f_x = function(x, y){
      result = rbind(0, 0.5*norm(y,type = "2")^2 )
      return(result)
    }

    f_y = function(x, y){
      result = t(t(A_tr %*% y - b_tr) %*% A_tr + x[2] * t(y))
      return(result)
    }

    g_x = function(x, y){
      result =  rbind(norm(y,type = "1"), 0)
      return(result)
    }

    # Proximal operator for theta
    prox_eta = function(x, y, theta){
      z = theta - eta * (f_y(x, theta) + (theta - y) / gamma)
      lambda = eta * matrix(rep(x[1], p_fea), nrow = p_fea)
      result = sign(z) * pmax((abs(z) - lambda), 0*ep)
    }

    # Proximal operator for y
    prox_beta = function(x, y, dky){
      z = y - beta * dky
      lambda = beta * matrix(rep(x[1], p_fea), nrow = p_fea)
      result = sign(z) * pmax((abs(z) - lambda), 0*ep)
    }


    # Array to store the results
    arrF = numeric(N) # F(x^{k+1},y^{k+1})
    res1 = numeric(N) #||x^{k+1}-x^k|| / ||x^k||
    res2 = numeric(N) #||y^{k+1}-y^k|| / ||y^k||
    res3 = numeric(N) #||theta^{k+1}-theta^k|| / ||theta^k||

    # Algorithm
    for (k in 1:N) {
      xk = x
      yk = y
      thetak = theta
      ck = c*(k)^p
      theta = prox_eta(x, y, theta)
      dkx = (1/ck) * F_x(x, y) + f_x(x, y) + g_x(x, y) - f_x(x, theta) - g_x(x, theta)
      x = pmax(x - alpha * dkx,0*e2)
      dky = (1/ck) * F_y(x, y) + f_y(x, y) - (y - theta)/gamma
      y = prox_beta(x, y, dky)

      # Convergence checking sequence
      res1[k] = norm(x - xk , "2") / norm(xk, "2")
      res2[k] = norm(y - yk, "2") / norm(yk, "2")
      res3[k] = norm(theta - thetak, "2") / norm(thetak, "2")
      arrF[k] = F(x, y) # Save current function value
      if (!is.na(res1[k]) && res1[k] < tol) {
        if(auto_tuning == FALSE){
          cat("Terminating at iteration", k, "\n")
        }
        break
      }
    }

    return(list(x = x, y = y, theta = theta, Xconv = res1, Yconv = res2, Thetaconv= res3, Fseq = arrF))
  }

  if(auto_tuning == TRUE){
    message("\n","Auto-hyperparameters-tuning is proceeding now.")

    iter <- 100
    T <- temperature

    pb <- progress_bar$new(
      total = iter,
      format = "  Finished :current/:total [:bar] :percent  remaining time :eta"
    )


    alpha.seq <- numeric(iter)
    beta.seq <- numeric(iter)
    eta.seq <- numeric(iter)
    value <- numeric(iter)

    alpha.seq[1] <- alpha
    beta.seq[1] <- beta
    eta.seq[1] <- eta

    result = main_fun(A_val, b_val, A_tr, b_tr, N, alpha = alpha.seq[1], beta = beta.seq[1], eta = eta.seq[1], gamma = gamma, c = c, p = p, tol = tol)
    value[1] <- result$Fseq[order(result$Fseq, decreasing = FALSE)[1]]



    set.seed(123)
    for (j in 2:iter) {
      T <- exp(-0.0001*j)
      alpha.seq[j] <- rtruncnorm(n = 1, a = 0, mean = alpha.seq[j-1], sd = 1e-3)
      beta.seq[j] <- rtruncnorm(n = 1, a = 0, mean = beta.seq[j-1], sd = 1e-6)
      eta.seq[j] <- rtruncnorm(n = 1, a = 0, mean = eta.seq[j-1], sd = 1e-6)
      result = main_fun(A_val, b_val, A_tr, b_tr, N, alpha = alpha.seq[j], beta = beta.seq[j], eta = eta.seq[j], gamma = gamma, c = c, p = p, tol = tol)
      candidate <- result$Fseq[order(result$Fseq, decreasing = FALSE)[1]]
      if(candidate > value[j-1] & runif(n = 1) > exp((value[j-1]-candidate)/T)){
        value[j] <- value[j-1]
      } else {
        value[j] <- candidate
      }
      pb$tick()
    }

    opt_index <- order(value)[1]

    cat("\n", "Auto-hyperparameters-tuning is done.")
    cat("\nFinal hyper-paramaters (alpha,beta,eta) are chosen as:",c(alpha.seq[opt_index], beta.seq[opt_index], eta.seq[opt_index]))

    return(main_fun(A_val, b_val, A_tr, b_tr, N, alpha = alpha.seq[opt_index], beta = beta.seq[opt_index], eta = eta.seq[opt_index], gamma, c, p, tol))

  } else{
    return(main_fun(A_val, b_val, A_tr, b_tr, N, alpha, beta, eta, gamma, c, p, tol))
  }


}
