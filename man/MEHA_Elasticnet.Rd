% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/MEHA_ElasticNet.R
\name{MEHA_Elasticnet}
\alias{MEHA_Elasticnet}
\title{Solving Elastic Net Problem Based on MEHA}
\usage{
MEHA_Elasticnet(
  A_val,
  b_val,
  A_tr,
  b_tr,
  N = 500,
  alpha = 0.001,
  beta = 1e-05,
  eta = 1e-05,
  gamma = 2,
  c = 10,
  p = 0.48,
  tol = 0.05,
  auto_tuning = FALSE,
  temperature = 0.1
)
}
\arguments{
\item{A_val}{Input feature matrix of the validation set, with dimensions n by p,
where n is the total number of validation samples and p is the dimension of
features. Each row represents an observation vector.}

\item{b_val}{Quantitative response variable of validation set.}

\item{A_tr}{Input feature matrix of training set, with dimension n' by p,
where n' is the total number of training samples and p is the dimension of features.}

\item{b_tr}{Quantitative response variable of training set.}

\item{N}{Total iterations of MEHA. Default is 500.}

\item{alpha}{Projection stepsize of \code{x}, which is fixed. Default is 0.001.
Specifically, you can edit the code and utilize the inverse power learning rate annealing strategy to dynamically adjust it.}

\item{beta}{Proximal gradient stepsize of \code{y} which is fixed. Default is 1e-5.
Specifically, you can edit the code and utilize the inverse power learning rate annealing strategy to dynamically adjust it.}

\item{eta}{Proximal gradient stepsize of \code{theta}, which is fixed.
Default is 1e-5.}

\item{gamma}{Moreau envelope parameter. Default is 2.}

\item{c}{\eqn{\underline{c}} in MEHA, which is used to generate the penalty parameter \eqn{c_k} in LV-HBA by \eqn{c_k = \underline{c}(k+1)^p}.
Default is 1.}

\item{p}{Default is 0.48.}

\item{tol}{Tolerance. IF \eqn{|x^{k + 1} - x^{k}|/|x^{k}| < tol}, then terminate the iteration, where \eqn{x^k} represents the upper-level variable.}

\item{auto_tuning}{When alpha, beta, eta are fixed, whether an auto-hyperparameter-tuning is needed.
Default is \code{FALSE}.}

\item{temperature}{Temperature of simulating annealing method for auto-
hyperparameter-tuning. Default is 0.1.}
}
\value{
\item{x}{The first value is \code{x1} (Lasso penalty strength), while the
second value is \code{x2} (Ridge penalty strength).}
\item{y}{The feature coefficient vector, of dimension p, where p is the
p is the dimension of features.}
\item{theta}{The moreau envelope parameter in MEHA, which is of the same scale as variable y}
\item{Xconv}{Describe the relative convergence situation of sequence x generated by MEHA,
which is computed by \eqn{||x^{k+1}-x^k|| / ||x^k||} based on l2-norm.}
\item{Yconv}{Describe the relative convergence situation of sequence y generated by MEHA,
which is computed by \eqn{||y^{k+1}-y^k|| / ||y^k||} based on l2-norm.}
\item{Thetaconv}{Describe the relative convergence situation of sequence theta generated by MEHA,
which is computed by \eqn{||theta^{k+1}-theta^k|| / ||theta^k||} based on l2-norm.}
\item{Fseq}{The upper function value sequence generated by the iteration based on validation set.}
}
\description{
Elastic Net combines the penalties of Lasso (L1 regularization)
and Ridge (L2 regularization) methods. Thus the model introduces two
hyperparameters: \code{x1}, which controls the strength of L1 regularization;
and \code{x2}, which controls the strength of L2 regularization.
The purpose of this function is to determine the optimal feature
coefficients \code{y} and the hyperparameters \code{x1} and \code{x2} of the
elastic net based on the input training and validation sets using MEHA.
}
\examples{
library(MASS)
p <- 600  # Number of features
M <- 30  # Number of groups
n_tr <- 100  # Number of training samples
n_val <- 100  # Number of validation samples
n_test <- 100  # Number of test samples
signal_to_noise_ratio <- 2  # Signal-to-noise ratio

# Generate correlation matrix
co_matrix <- diag(1, nrow = p, ncol = p)

# Generate A matrix
A <- MASS::mvrnorm(n = n_tr + n_val + n_test, mu = rep(0, p), Sigma = co_matrix)

# Generate beta vector
beta_1 <- matrix(rep(0, p / 3), ncol = 1)
for (i in 1:5) {
  beta_1[i, 1] <- i
}
beta <- rbind(beta_1, beta_1, beta_1)

# Generate epsilon
epsilon <- matrix(rnorm(n_tr + n_val + n_test), ncol = 1)

# Adjust sigma to achieve the specified signal-to-noise ratio
sigma <- norm(A \%*\% beta, type = "2") / (signal_to_noise_ratio * norm(epsilon, type = "2"))
b <- A \%*\% beta + sigma * epsilon

# Split data into training, validation, and test sets
col_indices <- sample(n_tr + n_val + n_test)
A_tr <- A[col_indices[1:n_tr], ]
b_tr <- b[col_indices[1:n_tr], ]
A_val <- A[col_indices[(n_tr + 1):(n_tr + n_val)], ]
b_val <- b[col_indices[(n_tr + 1):(n_tr + n_val)], ]
A_test <- A[col_indices[(n_tr + n_val + 1):(n_tr + n_val + n_test)], ]
b_test <- b[col_indices[(n_tr + n_val + 1):(n_tr + n_val + n_test)], ]


}
\references{
Liu, R., Liu, Z., Yao, W., Zeng, S., & Zhang, J. (2024).
"Rethinking Moreau Envelope for Nonconvex Bi-Level Optimization: A Single-loop and Hessian-free Solution Strategy."
Available at: https://openreview.net/forum?id=i6EtCiIK4a

\if{html}{\out{<div class="sourceCode">}}\preformatted{Gao, L., Ye, J. J., Yin, H., Zeng, S., & Zhang, J. (2022).
"Value function based difference-of-convex algorithm for bilevel hyperparameter selection problems."
Available at: https://proceedings.mlr.press/v162/gao22j.html
}\if{html}{\out{</div>}}
}
